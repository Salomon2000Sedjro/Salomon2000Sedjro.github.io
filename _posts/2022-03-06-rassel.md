---
layout: post
title:  "RANDOM SUBSPACE LEARNING WITH DATA DRIVEN WEIGHTING SCHEMES"
#date:   2021--28 09:53:02
categories: machine learning algorithms
permalink: /posts/rassel
image: "https://github.com/Salomon2000Sedjro/Salomon2000Sedjro.github.io/blob/fae3943cb42f4a8e2ef9d9eae764ea227bf4853f/photos/blog1_rassel1.jpg?raw=true"
---

![subspace](https://github.com/Salomon2000Sedjro/Salomon2000Sedjro.github.io/blob/fae3943cb42f4a8e2ef9d9eae764ea227bf4853f/photos/blog1_rassel1.jpg?raw=true)

## INTRODUCTION
In order to improve the precision of a regression or a classification function, in machine learning
it is beneficial to combine several estimators because it has been proven that an appropriate mix of
good core learners leads to a reduction in prediction error. This technique is known as ensemble learning
(aggregation). One of the ensemble learning methods is Random Adaptive Subspace Learning (RASSEL).	
What are the importances of RASSEL and how does it work?

![algorithm_layout](https://github.com/Salomon2000Sedjro/Salomon2000Sedjro.github.io/blob/48adef7a1e6114a7579d31b8afd48b37c23c333f/photos/blog1_rassel2.png?raw=true)

## IMPORTANCE OF RASSEL
Usually, it is hard for a traditional algorithm to build a regression model, or to classify the dataset
when it possesses a very small instances to features ratio (n<<p). The prediction problem becomes even
more difficult when this huge number of features are highly correlated.
To solve this problem, RASSEL guides the selection  of good candidate features from the dataset,
and therefore select the best base learners, and ultimately the ensemble yielding the lowest possible
prediction error.

## WEIGHTING SCHEMES
### Uniform scheme ?
In most typical random subspace learning algorithms, the features are selected according to an equally
likely scheme. One may therefore wonder if it is possible to choose the candidate features for with some
predictive benefits.
### Data-driven weighting schemes !
The proposed method of data-driven weighting schemes that I'm going to present, explores a variety of
weighting schemes for choosing the features, based on the statistical measures of relationship between
the response variable and each explanatory variable.

## How does RASSEL work ?
Like random forest and all other random subspace learning methods, RASSEL consists of building an ensemble of $L$ base learners noted here:

<img src="https://latex.codecogs.com/svg.image?\mathcal{G}_{RASSEL}&space;=&space;\{\widehat{g}^{(1)},...,\widehat{g}^{(l)},...,\widehat{g}^{(L)}\}&space;" />

and forming the ensemble prediction function as:

<img src="https://latex.codecogs.com/svg.image?\widehat{f}^{(L)}(.)&space;=&space;\dfrac{1}{L}\sum_{l=1}^{L}\widehat{g}^{(l)}(.)" />

## PREDICTION TASK
### For classification
In classification, we predict the class membership of
<img src="https://latex.codecogs.com/svg.image?x^{*}&space;\in&space;\mathcal{X}" />,
by using the ensemble predicting estimator:

<img src="https://latex.codecogs.com/svg.image?\widehat{f}^{(L)}(x^{*})&space;=&space;\underset{y\in\mathcal{Y}}{argmax}\biggl\{\sum_{i=1}^{L}\bigg(\textbf{1}_{(y=\widehat{g}^{(L)}(x^{*}))}\bigg)&space;\biggr\}" />

### For regression
Given
<img src="https://latex.codecogs.com/svg.image?x^{*}&space;\in&space;\mathcal{X}" />,
we predict its corresponding response using:

<img src="https://latex.codecogs.com/svg.image?\widehat{f}^{(L)}(x^{*})&space;=&space;\dfrac{1}{L}\sum_{l=1}^{L}\widehat{g}^{(l)}(x^{*})" />

## RASSEL Algorithm

![algorithm_layout](https://github.com/Salomon2000Sedjro/Salomon2000Sedjro.github.io/blob/61dbeec442f92d0692c87fc34a2f8bcb22ea7217/photos/algo1.PNG?raw=true)

One of the most crucial ingredients in the above proposed algorithm is the dimension q of the subspace, because its value has a strong bearing on an important aspect of  the correlation among the base learners.
The book $Random Forests, Machine Learning, L. Breiman, 45 (2001), 532. recommends reasonable values;

### If p<<n
For classification, <img src="https://latex.codecogs.com/svg.image?q=[\sqrt{p}]" /> and for regression, <img src="https://latex.codecogs.com/svg.image?q=[p/3]" />
### If p>>n
For classification, <img src="https://latex.codecogs.com/svg.image?q=min([n/5],[\sqrt{p}])" /> and for regression, <img src="https://latex.codecogs.com/svg.image?q=min([n/5],[p/3])" />

### Variance of the ensemble prediction function
<img src="https://latex.codecogs.com/svg.image?&space;&space;&space;\begin{align*}&space;&space;&space;&space;\mathbb{V}[\widehat{f}^{(L)}(.)]&space;&=&space;\mathbb{V}\biggl[\dfrac{1}{L}\sum_{l=1}^{L}\widehat{g}^{(l)}(.)\biggr]\\&space;&space;&space;&space;\&space;&=&space;\dfrac{1}{L^2}\biggl[\sum_{l=1}^{L}\mathbb{V}[\widehat{g}^{(l)}(.)]&plus;\sum_{l=1}^{L}\sum_{l'\ne&space;l}^{L}cov\bigg(\widehat{g}^{(l)}(.),\widehat{g}^{(l')}(.)\bigg)\biggr]\\&space;&space;&space;&space;\&space;&=&space;\dfrac{1}{L^2}\biggl[\sum_{l=1}^{L}\sigma^2&plus;\sum_{l=1}^{L}\sum_{l'\ne&space;l}^{L}\rho\sigma^2\biggr]\\&space;&space;&space;&space;\&space;&=&space;\dfrac{1}{L^2}\biggl[L\sigma^2&plus;L(L-1)\rho\sigma^2\biggr]\\&space;&space;&space;&space;\mathbb{V}[\widehat{f}^{(L)}(.)]&space;&space;&=&space;\dfrac{\sigma^2}{L}&plus;\dfrac{(L-1)}{L}\rho\sigma^2\\&space;&space;&space;&space;where\&space;\sigma^2&space;=&space;\mathbb{V}[\widehat{g}^{(L)}(.)]\&space;&and\&space;\rho\sigma^2&space;=&space;cov\bigg(\widehat{g}^{(l)}(.),\widehat{g}^{(l')}(.)\bigg)\&space;for\&space;l\ne&space;l'&space;&space;&space;\end{align*}" />

